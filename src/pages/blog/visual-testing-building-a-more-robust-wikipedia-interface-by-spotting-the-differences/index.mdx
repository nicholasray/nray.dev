---
title:
  "Visual Testing: Building A More Robust Wikipedia Interface By Spotting the
  Differences"
layout: "@layouts/BlogLayout.astro"
cover:
  filename: pixel-globe.jpg
  alt: "Pixelated globe on a yellow background"
  credit: "Photo by [SIMON LEE](https://unsplash.com/photos/sWI76WS6OMc)"
publishedAt: 2023-01-19
description:
  A look at how we leveraged visual testing to catch UI bugs when developing
  Wikipedia's new desktop experience.
---

import Image from "@components/BlogImage.astro";
import complexStates from "./assets/complex-states.png";
import referenceAndTest from "./assets/reference-and-test.png";
import testDiff from "./assets/test-diff.png";
import vector2022 from "./assets/vector-2022.png";

Have you ever played the
["Spot the Difference"](https://en.wikipedia.org/wiki/Spot_the_difference) game?
It's where you try to find slight differences between two very similar images.
When I review code in a nontrivial patch or pull request that changes a
website's HTML, CSS, or JavaScript, I feel like I'm playing this game. Sometimes
I win. Other times, I lose.

However, unlike the original game, in which the player tries to identify a known
number of innocuous differences, there are times when I can't tell how many
visual changes a code change might cause. And contrary to the original game,
where the consequences for not noticing the differences might result in losing a
spot on a leaderboard somewhere, the effects of not catching accidental changes
in code can result in degradations to the user experience, loss of
functionality, a significant amount of time spent fixing the issues, or worse.

<Image
  src={referenceAndTest}
  alt="Two very similar screenshots of an article with slight visual differences."
  caption="Can you spot the differences between these two images? See the end of this article for the answer!"
/>

In the past year, developers on the
[Web team](https://www.mediawiki.org/wiki/Readers/Web/Team) at the Wikimedia
Foundation, the nonprofit organization that operates Wikipedia, have been using
**visual testing** to help develop one of our most ambitious projects. This post
chronicles our experience.

## Development of Wikipedia's new skin

By March 2022, our team was about three-quarters of the way through a
three-year-long project to update Wikipedia's desktop skin, which controls the
appearance and layout of the page. The previous skin,
[Vector](https://www.mediawiki.org/wiki/Skin:Vector), had been the default look
on Wikipedia for 12 years. It was time for a better desktop experience.

<Image
  src={vector2022}
  alt="A screenshot of the new Vector-2022 skin on Wikipedia showing an article of a Bighorn Sheep."
  caption="The biggest update to Wikipedia's desktop look in over a decade: Vector-2022"
/>

At that point, we had incrementally built and released several features of our
new skin, [Vector 2022](https://www.mediawiki.org/wiki/Skin:Vector/2022), to
many pilot wikis worldwide. We developed a new header with a collapsible
sidebar, limited content width, improved search and language switching
experience, revamped user menu, and a sticky header. Each release brought us
closer to our goal of creating a more welcoming, inclusive, and user-friendly
interface.

**But we were also releasing a lot of visual regressions**.

Each feature we built was coupled with a significant risk that it would deviate
from its meticulously considered design spec as time passed. For example, one of
our most frequent regressions involved the search component. The dimensions of
its input kept changing in undesired ways, or the position of its magnifying
glass icon would frequently move to a less attractive location as if an
invisible magnet on the page was pulling it.

We fell into a cycle of changing code, causing visual regressions, spending time
fixing those regressions and then having similar ones occur in as little as a
few weeks.

<Image
  src={complexStates}
  alt="A grid showing some of the different states that the table of contents, page tools, and the main menu could have."
  caption="A grid showing some of the different states that the table of contents, page tools, and the main menu could have."
/>

In addition, our UI was only getting more complex. For example, we wanted
certain elements only to be visible when logged in and only when the viewport
was above a specific breakpoint. Also, the content should be centered on the
page, but only when the table of contents isn't pinned next to it. Speaking of
the table of contents, when the user clicks its "hide" button, it should
collapse into the page toolbar, but only if the user hasn't scrolled past the
page's first heading to make the sticky header appear. Otherwise, it should be
in the sticky header. Unless, of course, the viewport was at a small width. If
that happens, the table of contents menu button should be at the top of the
viewport but without the sticky header showing.

Using the [Jest Testing Framework](https://jestjs.io/), we tried writing unit
tests for as much of the JavaScript logic involved in these situations as
possible. In addition, we leveraged Jest's
[snapshot feature](https://jestjs.io/docs/snapshot-testing#:~:text=Snapshot%20tests%20are%20a%20very,file%20stored%20alongside%20the%20test.),
which compares changes in the outputted DOM to detect any undesired class or
element changes.

However, there was still a gap in test coverage. Much of the skin's complexity
did not reside in JavaScript or HTML but in CSS, and this gap was getting
exposed more and more frequently. We needed something more than unit and
integration tests could provide. Something that could test what users _see_.

## Visual testing pixel by Pixel

From March through June 2022, we
[experimented with visual testing](https://phabricator.wikimedia.org/T302246)
and tried incorporating it into our workflow. At its most basic level, a visual
test, also known as a "visual regression test", involves:

1. Capturing a screenshot of a page or element (known as the "reference" or
   "baseline" screenshot).
2. Making changes to the code
3. Capturing another screenshot of that same page or element (commonly known as
   the "test" screenshot)
4. Comparing and highlighting the difference between the two screenshots

Not knowing whether this experiment would be helpful, we leveraged existing
open-source visual regression testing
[software](https://github.com/garris/BackstopJS) to point at a
[Dockerized instance of MediaWiki](https://www.mediawiki.org/wiki/MediaWiki-Docker)
(the software that powers Wikipedia), developed a CLI and logic that made it
easier to compare two different states of MediaWiki, and called the tool
[Pixel](https://github.com/wikimedia/pixel).

For MediaWiki to resemble anything remotely close to the production environment
that users see on English Wikipedia requires many git submodules in the form of
[extensions](https://www.mediawiki.org/wiki/Manual:Extensions). In addition to
the core code of MediaWiki, each of these extensions can cause visual changes to
the page.

Instead of storing and managing hundreds (or more) reference images under source
control (a common strategy with visual testing) and asking maintainers of these
extensions to keep the reference images up-to-date, we made Pixel compare one
state of MediaWiki with another state and produce a report that shows the visual
differences.

The two most common workflows are:

1. Comparing the `HEAD` of the `master` branches with a
   [Gerrit patch](https://gerrit.wikimedia.org/). This is primarily used during
   code review.
2. Comparing the latest release branches of MediaWiki with the `HEAD` of
   `master`. A new release of Wikipedia usually gets deployed every week. By
   comparing the last release with `master`, we can _roughly_ see the visual
   changes the next release will introduce. We publish these reports for anyone
   to view at [pixel.wmcloud.org](https://pixel.wmcloud.org/).

## Seeing the impact

Within the first week of our team's use of visual testing, we caught unintended
changes during the code review process using Pixel. In addition, a lot of time
usually spent manually testing visible changes on a local server was replaced by
our visual tests doing that work for us in a matter of minutes. As a result,
confidence in releases also increased.

The benefits we saw from visual testing continue today.
[We frequently catch unintended visual changes](https://www.mediawiki.org/wiki/Reading/Web/Release_timeline/Visual_Changes)
with our tests and found it valuable enough to continue using it in our
workflow. Frankly, I have a hard time reviewing any code without it.

There are several drawbacks to our approach as well:

- Because our tests compare each pixel, they can flag changes that might
  otherwise be imperceptible to humans and be noisy. Usually, this can be
  alleviated by adjusting the difference threshold to ignore subtle changes.
- Writing tests that capture the page at the right moment can be a pain. Visual
  tests need to be stable. All of our tests
  [fast-forward through any animation on the page](https://github.com/wikimedia/pixel/blob/main/src/engine-scripts/puppet/fastForwardAnimations.js)
  and
  [wait for idle moments in the network and main thread](https://github.com/wikimedia/pixel/blob/474fd24e81ddb39f60b09ca2ed69e67c3d4f8774/src/engine-scripts/puppet/fastForwardAnimations.js)
  before taking any screenshot. This works _most_ of the time.
- We sometimes have issues
  [determining whether a visual change is intentional and who caused it](https://phabricator.wikimedia.org/T320303).

We still have a long way to go, but we are improving. Our "desktop" test suite
captures 140 screenshots and tests five different viewport widths. We still have
visual regressions that slip into production. However, many of these result from
_not_ having the necessary visual tests in place for specific scenarios rather
than a failure of the existing tests.

<Image
  src={testDiff}
  alt="An image diff of two very similar screenshots of an article. Differences include a bigger search input with a misaligned icon, missing arrow icons in the table of contents, and a missing arrow icon next to the language switcher."
  caption="Highlighted in pink, there are three visual regressions from the set of images shown in the introduction of this article."
/>

So we will continue increasing the coverage of our visual test suite with more
tests and more scenarios. With each new visual test we write, we develop an
unfair advantage in the "Spot the Difference" game. And the difference between
where we were before visual testing and now feels like a step in the right
direction.

## Resources

- If you're a Wikimedia developer interested in using visual testing on your
  team, you might want to check out [Pixel](https://github.com/wikimedia/pixel)

- If you're interested in using visual testing outside of Wikimedia projects,
  you might want to check out [Playwright](https://playwright.dev/) or
  [BackstopJS](https://github.com/garris/BackstopJS)

- To learn more about the first update to Wikipedia's look on desktop in over a
  decade, check out:
  - [The new Wikipedia appearance that took the whole village](https://jdlrobson.com/posts/2023-01-17_the-new-wikipedia-appearance-that-took-a-whole-village)
  - [Wikipedia’s new look makes it easier to use for everyone](https://diff.wikimedia.org/2023/01/18/wikipedias-new-look-makes-it-easier-to-use-for-everyone/)
  - [The story behind Wikipedia’s upcoming new look](https://diff.wikimedia.org/2021/08/12/the-story-behind-wikipedias-upcoming-new-look/)
